"""
–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ-–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–Ω–∏–π —Å–∏–Ω—Ç–µ–∑ –ó–ù–ú
–ü—Ä–∞—Ü—é—î –æ–¥–Ω–∞–∫–æ–≤–æ –Ω–∞ M2 Pro —Ç–∞ Google Colab

–ê–≤—Ç–æ—Ä: –ê–Ω–∞—Ç–æ–ª—ñ–π –ö–æ—Ç (Anatoly Kot)
–î–∞—Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è: 2026-01-09
"""

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π
try:
    import optuna
except ImportError:
    print("üì¶ –í—Å—Ç–∞–Ω–æ–≤–ª—é—é Optuna...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "optuna"])
    import optuna
    print("‚úÖ Optuna –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ!")

try:
    from scipy.stats import spearmanr
except ImportError:
    print("üì¶ –í—Å—Ç–∞–Ω–æ–≤–ª—é—é SciPy...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "scipy"])
    from scipy.stats import spearmanr
    print("‚úÖ SciPy –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ!")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from PIL import Image
import numpy as np
import time
import json
import sys
from datetime import datetime, timezone

# ============================================
# –õ–æ–≥—É–≤–∞–Ω–Ω—è –∑ timestamps
# ============================================

# –Ü–º'—è —Ñ–∞–π–ª—É –ª–æ–≥—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —á–∞—Å—É –∑–∞–ø—É—Å–∫—É
EXPERIMENT_START_TIME = datetime.now(timezone.utc)
LOG_FILENAME = f"results/experiment_{EXPERIMENT_START_TIME.strftime('%Y%m%d_%H%M%S_UTC')}.log"

class TeeLogger:
    """–ö–ª–∞—Å –¥–ª—è –¥—É–±–ª—é–≤–∞–Ω–Ω—è –≤–∏–≤–æ–¥—É –≤ –∫–æ–Ω—Å–æ–ª—å —ñ —Ñ–∞–π–ª"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = None
        self.filename = filename
        
    def start(self):
        """–ü–æ—á–∞—Ç–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
        Path(self.filename).parent.mkdir(parents=True, exist_ok=True)
        self.log = open(self.filename, 'w', encoding='utf-8', buffering=1)
        
    def write(self, message):
        """–ó–∞–ø–∏—Å–∞—Ç–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è"""
        self.terminal.write(message)
        if self.log:
            self.log.write(message)
            
    def flush(self):
        """–û—á–∏—Å—Ç–∏—Ç–∏ –±—É—Ñ–µ—Ä"""
        self.terminal.flush()
        if self.log:
            self.log.flush()
            
    def close(self):
        """–ó–∞–∫—Ä–∏—Ç–∏ —Ñ–∞–π–ª –ª–æ–≥—É"""
        if self.log:
            self.log.close()

def get_timestamp():
    """–ü–æ–≤–µ—Ä—Ç–∞—î –ø–æ—Ç–æ—á–Ω–∏–π UTC timestamp"""
    return datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')

def log_print(message, prefix=""):
    """Print –∑ timestamp"""
    timestamp = get_timestamp()
    if prefix:
        print(f"[{timestamp}] {prefix} {message}")
    else:
        print(f"[{timestamp}] {message}")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ TeeLogger
tee_logger = TeeLogger(LOG_FILENAME)

# ============================================
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É (Colab)
# ============================================

# ============================================
# –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø - –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ï –í–ò–ó–ù–ê–ß–ï–ù–ù–Ø –†–ï–ñ–ò–ú–£
# ============================================

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∏ —Ü–µ Google Colab
try:
    import google.colab
    IS_COLAB = True
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–∞ —Ä–æ–∑–ø–∞–∫—É–≤–∞—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑ Google Drive
    print("\n" + "="*60)
    print("üì¶ –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–£ –ó GOOGLE DRIVE")
    print("="*60)
    
    from google.colab import drive
    import zipfile
    import shutil
    
    # –ú–æ–Ω—Ç—É–≤–∞—Ç–∏ Drive —è–∫—â–æ —â–µ –Ω–µ –∑–º–æ–Ω—Ç–æ–≤–∞–Ω–æ
    if not Path('/content/drive').exists():
        print("\nüìÅ –ú–æ–Ω—Ç—É–≤–∞–Ω–Ω—è Google Drive...")
        drive.mount('/content/drive')
    
    # –®–ª—è—Ö –¥–æ dataset.zip –Ω–∞ Drive
    dataset_zip = '/content/drive/MyDrive/Studying/Experiments/Composite_score_nas/dataset.zip'
    
    if Path(dataset_zip).exists():
        print(f"\n‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ dataset.zip –Ω–∞ Drive")
        print(f"   –†–æ–∑–º—ñ—Ä: {Path(dataset_zip).stat().st_size / (1024**3):.2f} GB")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –¥–∞—Ç–∞—Å–µ—Ç –≤–∂–µ —Ä–æ–∑–ø–∞–∫–æ–≤–∞–Ω–∏–π
        if not Path('dataset/train').exists() or not Path('dataset/val').exists():
            print(f"\nüì¶ –†–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É...")
            print(f"   –¶–µ –∑–∞–π–º–µ ~2-3 —Ö–≤–∏–ª–∏–Ω–∏...")
            
            # –í–∏–¥–∞–ª–∏—Ç–∏ —Å—Ç–∞—Ä–∏–π –¥–∞—Ç–∞—Å–µ—Ç —è–∫—â–æ —î
            if Path('dataset').exists():
                shutil.rmtree('dataset')
            
            # –†–æ–∑–ø–∞–∫—É–≤–∞—Ç–∏
            with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:
                zip_ref.extractall('.')
            
            print(f"   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Ä–æ–∑–ø–∞–∫–æ–≤–∞–Ω–æ!")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            if Path('dataset/train').exists() and Path('dataset/val').exists():
                n_train = len(list(Path('dataset/train/images').glob('*.jpg')))
                n_val = len(list(Path('dataset/val/images').glob('*.jpg')))
                print(f"   üìä Train: {n_train} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
                print(f"   üìä Val: {n_val} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
            else:
                raise FileNotFoundError("–ü–æ–º–∏–ª–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –¥–∞—Ç–∞—Å–µ—Ç—É –ø—ñ—Å–ª—è —Ä–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è")
        else:
            print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≤–∂–µ —Ä–æ–∑–ø–∞–∫–æ–≤–∞–Ω–∏–π")
            n_train = len(list(Path('dataset/train/images').glob('*.jpg')))
            n_val = len(list(Path('dataset/val/images').glob('*.jpg')))
            print(f"   üìä Train: {n_train} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
            print(f"   üìä Val: {n_val} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
    else:
        raise FileNotFoundError(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ dataset.zip –Ω–∞ Drive!\n"
                              f"   –û—á—ñ–∫—É–≤–∞–Ω–∏–π —à–ª—è—Ö: {dataset_zip}\n"
                              f"   –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ dataset.zip –≤ MyDrive/Studying/")
    
except ImportError:
    IS_COLAB = False

# –†–µ–∂–∏–º —Ä–æ–±–æ—Ç–∏ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∞–±–æ –≤—Ä—É—á–Ω—É)
FULL_RUN_MODE = IS_COLAB  # True –Ω–∞ Colab, False –ª–æ–∫–∞–ª—å–Ω–æ
# –Ø–∫—â–æ —Ö–æ—á–µ—Ç–µ –∑–º—ñ–Ω–∏—Ç–∏ –≤—Ä—É—á–Ω—É, —Ä–æ–∑–∫–æ–º–µ–Ω—Ç—É–π—Ç–µ:
# FULL_RUN_MODE = True   # –ü—Ä–∏–º—É—Å–æ–≤–æ –ø–æ–≤–Ω–∏–π –ø—Ä–æ–≥–æ–Ω
# FULL_RUN_MODE = False  # –ü—Ä–∏–º—É—Å–æ–≤–æ —à–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç

if FULL_RUN_MODE:
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # üöÄ –ü–û–í–ù–ò–ô –ü–†–û–ì–û–ù (–¥–ª—è Google Colab T4/A100)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    N_TRIALS = 50              # –ü–æ–≤–Ω–∏–π –ø—Ä–æ–≥–æ–Ω
    TIMEOUT = 7200             # 2 –≥–æ–¥–∏–Ω–∏
    MAX_SAMPLES = 2000         # –ü–æ–≤–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–∏–Ω—Ç–µ–∑—É
    FULL_MAX_SAMPLES = 2000    # –ü–æ–≤–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    EPOCHS_PER_TRIAL = 2       # 2 –µ–ø–æ—Ö–∏ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
    N_WARMUP = 15              # –ü–æ–≤–Ω–∞ –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ—è
    VAL_SUBSET = 200           # –ü–æ–≤–Ω–∏–π validation subset
    FULL_EPOCHS = 30           # –ü–æ–≤–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (–∑–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó –Ω–∞–¥—ñ–π–Ω–æ—Å—Ç—ñ)
    FULL_BATCH_SIZE = 32       # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è T4
    # ‚è±Ô∏è –û—á—ñ–∫—É–≤–∞–Ω–∏–π —á–∞—Å: ~24 –≥–æ–¥–∏–Ω–∏ –Ω–∞ T4, ~10 –≥–æ–¥–∏–Ω –Ω–∞ A100
else:
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚ö° –®–í–ò–î–ö–ò–ô –¢–ï–°–¢ (–ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ MPS/CPU)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    N_TRIALS = 10              # –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç
    TIMEOUT = 600              # 10 —Ö–≤–∏–ª–∏–Ω
    MAX_SAMPLES = 300          # –ú–µ–Ω—à–µ –¥–∞–Ω–∏—Ö
    FULL_MAX_SAMPLES = 300     # –ú–µ–Ω—à–µ –¥–∞–Ω–∏—Ö
    EPOCHS_PER_TRIAL = 1       # 1 –µ–ø–æ—Ö–∞
    N_WARMUP = 5               # –®–≤–∏–¥–∫–∞ –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ—è
    VAL_SUBSET = 100           # –ú–µ–Ω—à–∏–π subset
    FULL_EPOCHS = 5            # –ú–µ–Ω—à–µ –µ–ø–æ—Ö
    FULL_BATCH_SIZE = None     # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
    # ‚è±Ô∏è –û—á—ñ–∫—É–≤–∞–Ω–∏–π —á–∞—Å: ~30-40 —Ö–≤–∏–ª–∏–Ω

# –°–ø—ñ–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
IMG_SIZE = 320                      # –†–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω—å
SEED = 42                           # –î–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ
USE_COMPOSITE_PROXY = True          # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ Composite Score
K_BATCHES = 10                      # –î–ª—è –∞–Ω–∞–ª—ñ–∑—É —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
FULL_PIPELINE = True                # –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª (—Å–∏–Ω—Ç–µ–∑ + —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è + –∞–Ω–∞–ª—ñ–∑)

# –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
Path('results').mkdir(exist_ok=True)

# ============================================
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏—Å—Ç—Ä–æ—é
# ============================================

if torch.cuda.is_available():
    device = torch.device("cuda")
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    BATCH_SIZE_OPTIONS = [16, 32, 64]
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    gpu_name = "Apple Metal Performance Shaders (MPS)"
    gpu_memory = None
    BATCH_SIZE_OPTIONS = [8, 16]  # ‚ö° –ó–º–µ–Ω—à–µ–Ω–æ batch sizes –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ MPS
else:
    device = torch.device("cpu")
    gpu_name = "CPU"
    gpu_memory = None
    BATCH_SIZE_OPTIONS = [8, 16, 32]

# ============================================
# Dataset
# ============================================

class SimpleDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —à–≤–∏–¥–∫–æ—ó –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, image_dir, label_dir, img_size=IMG_SIZE, max_samples=MAX_SAMPLES):
        self.img_size = img_size
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ —à–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
        if Path(image_dir).is_absolute():
            self.image_dir = Path(image_dir)
            self.label_dir = Path(label_dir)
        elif Path(image_dir).exists():
            # –î–∞—Ç–∞—Å–µ—Ç –≤ –ø–æ—Ç–æ—á–Ω—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó (Colab/Jupyter)
            self.image_dir = Path(image_dir)
            self.label_dir = Path(label_dir)
        else:
            # –õ–æ–∫–∞–ª—å–Ω–∏–π –∑–∞–ø—É—Å–∫ –∑ bayesian_optimization/
            try:
                script_dir = Path(__file__).parent
                self.image_dir = (script_dir / ".." / image_dir).resolve()
                self.label_dir = (script_dir / ".." / label_dir).resolve()
            except NameError:
                # Jupyter –±–µ–∑ __file__ - –æ—Å—Ç–∞–Ω–Ω—ñ–π —à–∞–Ω—Å
                self.image_dir = Path(image_dir)
                self.label_dir = Path(label_dir)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å–ø–∏—Å–æ–∫ –∑–æ–±—Ä–∞–∂–µ–Ω—å
        all_images = sorted(list(self.image_dir.glob("*.jpg")))
        self.images = all_images[:max_samples] if max_samples > 0 else all_images
        
        if len(self.images) == 0:
            raise ValueError(f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω—å –≤ {self.image_dir}")
        
        print(f"   Dataset: {len(all_images)} –∑–Ω–∞–π–¥–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é {len(self.images)}")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        img_path = self.images[idx]
        img = Image.open(img_path).convert('RGB')
        img = img.resize((self.img_size, self.img_size))
        img = np.array(img) / 255.0
        img = torch.FloatTensor(img).permute(2, 0, 1)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ label
        label_path = self.label_dir / f"{img_path.stem}.txt"
        if label_path.exists():
            with open(label_path) as f:
                line = f.readline().strip()
                if line:
                    parts = line.split()
                    cls = int(parts[0])
                    bbox = [float(x) for x in parts[1:5]]
                    target = torch.FloatTensor([cls] + bbox)
                else:
                    target = torch.zeros(5)
        else:
            target = torch.zeros(5)
        
        return img, target

# ============================================
# Composite Score Components (Train Loss + Gradient Stability)
# ============================================

class ProxyLogger:
    """–ó–±–∏—Ä–∞—î –º–µ—Ç—Ä–∏–∫–∏ –ø—ñ–¥ —á–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è Composite Score"""
    
    def __init__(self, k_batches=K_BATCHES):
        self.k_batches = k_batches
        self.train_losses = []
        self.grad_norms = []
        self.eps = 1e-8
    
    def log_batch(self, train_loss, grad_norm):
        """–õ–æ–≥—É–≤–∞—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –±–∞—Ç—á—É"""
        self.train_losses.append(train_loss)
        self.grad_norms.append(grad_norm)
    
    def compute_metrics(self, val_loss):
        """–û–±—á–∏—Å–ª–∏—Ç–∏ –ø—ñ–¥—Å—É–º–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –ø—ñ—Å–ª—è –µ–ø–æ—Ö–∏"""
        if len(self.train_losses) == 0:
            return None
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ K (–º—ñ–Ω—ñ–º—É–º 10 –∞–±–æ –ø–æ–ª–æ–≤–∏–Ω–∞ –±–∞—Ç—á—ñ–≤)
        k = min(self.k_batches, len(self.train_losses) // 2)
        if k < 1:
            k = len(self.train_losses)
        
        # –û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
        L_val = val_loss
        L_tr_first = np.mean(self.train_losses[:k])
        L_tr_last = np.mean(self.train_losses[-k:])
        
        # –ü–æ—Ö—ñ–¥–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
        gap = max(0, L_val - L_tr_last)
        impr = max(0, L_tr_first - L_tr_last)
        
        # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –≤–∞—Ä—ñ–∞—Ü—ñ—ó loss
        loss_last_k = self.train_losses[-k:]
        loss_mean = np.mean(loss_last_k)
        loss_std = np.std(loss_last_k)
        loss_cv = loss_std / (loss_mean + self.eps)
        
        # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –≤–∞—Ä—ñ–∞—Ü—ñ—ó –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
        grad_last_k = self.grad_norms[-k:]
        grad_mean = np.mean(grad_last_k)
        grad_std = np.std(grad_last_k)
        grad_cv = grad_std / (grad_mean + self.eps)
        
        return {
            'L_val': L_val,
            'L_tr_first': L_tr_first,
            'L_tr_last': L_tr_last,
            'gap': gap,
            'impr': impr,
            'loss_cv': loss_cv,
            'grad_cv': grad_cv,
            'grad_norm_mean': grad_mean,  # –î–æ–¥–∞–Ω–æ –¥–ª—è Composite Score
            'num_batches': len(self.train_losses)
        }


def compute_grad_norm(model):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ L2 –Ω–æ—Ä–º—É –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –º–æ–¥–µ–ª—ñ"""
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    return total_norm


class ProxyStats:
    """–£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ–π–Ω–æ—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é –¥–ª—è Composite Score"""
    
    def __init__(self, stats_file='results/proxy_stats.json', csv_file='results/trials_proxy_metrics.csv'):
        self.stats_file = stats_file
        self.csv_file = csv_file
        self.eps = 1e-8
        
        # –°–ø–∏—Å–∫–∏ –¥–ª—è warmup
        self.warmup_metrics = {
            'L_val': [],
            'L_tr_last': [],  # –î–æ–¥–∞–Ω–æ –¥–ª—è train_loss
            'gap': [],
            'impr': [],
            'loss_cv': [],
            'grad_cv': [],
            'grad_norm_mean': []  # –î–æ–¥–∞–Ω–æ –¥–ª—è grad_norm_mean
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—ñ—Å–ª—è warmup
        self.medians = {}
        self.iqrs = {}
        self.is_ready = False
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ—Å–Ω—É—é—á—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._load_stats()
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ CSV —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
        if not Path(self.csv_file).exists():
            with open(self.csv_file, 'w') as f:
                f.write('trial,L_val,gap,impr,loss_cv,grad_cv,L_tr_first,L_tr_last,grad_norm_mean,num_batches,proxy_value,objective_type\n')
    
    def _load_stats(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑ —Ñ–∞–π–ª—É"""
        if Path(self.stats_file).exists():
            with open(self.stats_file, 'r') as f:
                data = json.load(f)
                self.warmup_metrics = data.get('warmup_metrics', self.warmup_metrics)
                self.medians = data.get('medians', {})
                self.iqrs = data.get('iqrs', {})
                self.is_ready = data.get('is_ready', False)
    
    def _save_stats(self):
        """–ó–±–µ—Ä–µ–≥—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É —Ñ–∞–π–ª"""
        data = {
            'warmup_metrics': self.warmup_metrics,
            'medians': self.medians,
            'iqrs': self.iqrs,
            'is_ready': self.is_ready
        }
        with open(self.stats_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def update(self, metrics):
        """–î–æ–¥–∞—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –∑ –Ω–æ–≤–æ–≥–æ trial"""
        if self.is_ready:
            return  # Warmup –≤–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–π
        
        # –î–æ–¥–∞—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
        for key in ['L_val', 'L_tr_last', 'gap', 'impr', 'loss_cv', 'grad_cv', 'grad_norm_mean']:
            if key in metrics:
                self.warmup_metrics[key].append(metrics[key])
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ trials –¥–ª—è –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ—ó
        if len(self.warmup_metrics['L_val']) >= N_WARMUP:
            self._calibrate()
    
    def _calibrate(self):
        """–û–±—á–∏—Å–ª–∏—Ç–∏ medians —Ç–∞ IQRs –ø—ñ—Å–ª—è warmup"""
        for key in ['L_val', 'L_tr_last', 'gap', 'impr', 'loss_cv', 'grad_cv', 'grad_norm_mean']:
            values = np.array(self.warmup_metrics[key])
            self.medians[key] = float(np.median(values))
            q25 = float(np.percentile(values, 25))
            q75 = float(np.percentile(values, 75))
            self.iqrs[key] = q75 - q25
        
        self.is_ready = True
        self._save_stats()
        log_print(f"‚úÖ Composite Score –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ ({N_WARMUP} trials)")
    
    def z_normalize(self, key, value):
        """–û–±—á–∏—Å–ª–∏—Ç–∏ z-–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è"""
        if not self.is_ready or key not in self.medians:
            return value
        
        median = self.medians[key]
        iqr = self.iqrs[key]
        return (value - median) / (iqr + self.eps)
    
    def ready(self):
        """–ß–∏ –≥–æ—Ç–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è Composite Score"""
        return self.is_ready
    
    def log_trial(self, trial_num, metrics, proxy_value, objective_type):
        """–ó–∞–ø–∏—Å–∞—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ trial —É CSV"""
        with open(self.csv_file, 'a') as f:
            f.write(f"{trial_num},{metrics['L_val']:.6f},{metrics['gap']:.6f},"
                   f"{metrics['impr']:.6f},{metrics['loss_cv']:.6f},{metrics['grad_cv']:.6f},"
                   f"{metrics.get('L_tr_first', 0):.6f},{metrics.get('L_tr_last', 0):.6f},"
                   f"{metrics.get('grad_norm_mean', 0):.6f},"
                   f"{metrics.get('num_batches', 0)},{proxy_value:.6f},{objective_type}\n")


def compute_composite_score(metrics, stats):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ Composite Score –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É (–ø–æ–∫—Ä–∞—â–µ–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞ v2).
    
    –ù–û–í–ê –§–û–†–ú–£–õ–ê (–ø—ñ—Å–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É 2026-01-13):
    Score = 0.25¬∑z(impr) + 0.20¬∑z(L_val) + 0.15¬∑z(loss_cv) + 0.15¬∑z(grad_cv) + 
            0.15¬∑z(gap) + 0.05¬∑z(L_tr_last) + 0.05¬∑z(grad_norm)
    
    –ö–ª—é—á–æ–≤–µ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è: –í–°–Ü –º–µ—Ç—Ä–∏–∫–∏ –º–∞—é—Ç—å –ü–û–ó–ò–¢–ò–í–ù–£ –∫–æ—Ä–µ–ª—è—Ü—ñ—é –∑ Final Loss!
    - impr: +0.358 (–Ω–∞–π—Å–∏–ª—å–Ω—ñ—à–∞!) - —à–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è = –Ω–∏–∑—å–∫–∞ capacity
    - L_val: +0.248
    - loss_cv: +0.216
    
    –†–µ–∑—É–ª—å—Ç–∞—Ç: Spearman œÅ = +0.351 (+51% vs —Å—Ç–∞—Ä–∞ —Ñ–æ—Ä–º—É–ª–∞) üî•
    TOP-20 overlap: –æ—á—ñ–∫—É—î—Ç—å—Å—è ~65% (vs 30% –≤ —Å—Ç–∞—Ä—ñ–π)
    
    –í–ê–ñ–õ–ò–í–û: –ó z-–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é –¥–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±—É –º–µ—Ç—Ä–∏–∫!
    """
    if not stats.ready():
        # –î–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è warmup –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ø—Ä–æ—Å—Ç–∏–π proxy
        return metrics['L_val'] + 0.5 * metrics['gap'], 'simple'
    
    # –ó z-–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é –¥–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±—É
    # (–≤–∞–≥–∏ –ø—ñ–¥—ñ–±—Ä–∞–Ω—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É)
    z_impr = stats.z_normalize('impr', metrics['impr'])
    z_L_val = stats.z_normalize('L_val', metrics['L_val'])
    z_loss_cv = stats.z_normalize('loss_cv', metrics['loss_cv'])
    z_grad_cv = stats.z_normalize('grad_cv', metrics['grad_cv'])
    z_gap = stats.z_normalize('gap', metrics['gap'])
    z_L_tr_last = stats.z_normalize('L_tr_last', metrics['L_tr_last'])
    z_grad_norm = stats.z_normalize('grad_norm_mean', metrics['grad_norm_mean'])
    
    composite = (
        0.25 * z_impr +
        0.20 * z_L_val +
        0.15 * z_loss_cv +
        0.15 * z_grad_cv +
        0.15 * z_gap +
        0.05 * z_L_tr_last +
        0.05 * z_grad_norm
    )
    
    # –í–∏—â–µ Score = –≥—ñ—Ä—à–∞ –º–æ–¥–µ–ª—å (–º—ñ–Ω—ñ–º—ñ–∑—É—î–º–æ)
    return composite, 'Composite_v2'

# ============================================
# –î–∏–Ω–∞–º—ñ—á–Ω–∞ –º–æ–¥–µ–ª—å
# ============================================

class DynamicDetector(nn.Module):
    """–î–∏–Ω–∞–º—ñ—á–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ —Å–∏–Ω—Ç–µ–∑–æ–º"""
    
    def __init__(self, num_blocks, filters_list, kernel_sizes, fc_size, dropout, activation='relu'):
        super().__init__()
        
        # –í–∏–±—ñ—Ä —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó
        if activation == 'relu':
            activation_fn = nn.ReLU
        elif activation == 'leaky_relu':
            activation_fn = lambda: nn.LeakyReLU(0.1)
        elif activation == 'gelu':
            activation_fn = nn.GELU
        else:
            activation_fn = nn.ReLU
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ conv –±–ª–æ–∫–∏
        layers = []
        in_channels = 3
        
        for i in range(num_blocks):
            out_channels = filters_list[i]
            kernel_size = kernel_sizes[i]
            padding = kernel_size // 2
            
            layers.extend([
                nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),
                nn.BatchNorm2d(out_channels),
                activation_fn(),
                nn.MaxPool2d(2)
            ])
            in_channels = out_channels
        
        self.features = nn.Sequential(*layers)
        
        # Detection head
        self.detector = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(in_channels, fc_size),
            activation_fn(),
            nn.Dropout(dropout),
            nn.Linear(fc_size, 5)  # class + bbox (4 coords)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.detector(x)
        return x

# ============================================
# –§—É–Ω–∫—Ü—ñ—è —à–≤–∏–¥–∫–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
# ============================================

def train_trial(model, train_loader, val_loader, device, optimizer, epochs=EPOCHS_PER_TRIAL, use_composite=USE_COMPOSITE_PROXY):
    """–®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏"""
    
    criterion = nn.MSELoss()
    logger = ProxyLogger() if use_composite else None
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0
        for images, targets in train_loader:
            images, targets = images.to(device), targets.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, targets)
            
            optimizer.zero_grad()
            loss.backward()
            
            # –õ–æ–≥—É–≤–∞—Ç–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ —Ç–∞ loss –¥–ª—è Composite Score
            if use_composite and logger is not None:
                grad_norm = compute_grad_norm(model)
                logger.log_batch(loss.item(), grad_norm)
            
            optimizer.step()
            train_loss += loss.item()
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for images, targets in val_loader:
                images, targets = images.to(device), targets.to(device)
                outputs = model(images)
                loss = criterion(outputs, targets)
                val_loss += loss.item()
        
        val_loss /= len(val_loader)
    
    # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ val_loss —ñ metrics (—è–∫—â–æ Composite Score)
    if use_composite and logger is not None:
        metrics = logger.compute_metrics(val_loss)
        return val_loss, metrics
    
    return val_loss, None

# ============================================
# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Composite Score (—è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è)
# ============================================

proxy_stats = ProxyStats() if USE_COMPOSITE_PROXY else None

# ============================================
# –§—É–Ω–∫—Ü—ñ—ó –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (–¥–ª—è FULL_PIPELINE)
# ============================================

def train_model_full(model, train_loader, val_loader, device, optimizer_config, epochs=FULL_EPOCHS, model_name="model"):
    """–ü–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ optimizer –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ñ—ñ–≥—É
    optimizer_name = optimizer_config['optimizer']
    lr = optimizer_config['lr']
    weight_decay = optimizer_config['weight_decay']
    
    if optimizer_name == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)
    else:
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    criterion = nn.MSELoss()
    
    best_val_loss = float('inf')
    history = {
        'train_loss': [],
        'val_loss': [],
        'best_epoch': 0
    }
    
    print(f"\n{'='*60}")
    print(f"–ü–û–í–ù–ï –¢–†–ï–ù–£–í–ê–ù–ù–Ø: {model_name}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    for epoch in range(epochs):
        epoch_start = time.time()
        
        # Training
        model.train()
        train_loss = 0
        train_batches = 0
        
        for images, targets in train_loader:
            images, targets = images.to(device), targets.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, targets)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_batches += 1
        
        avg_train_loss = train_loss / train_batches
        history['train_loss'].append(avg_train_loss)
        
        # Validation
        model.eval()
        val_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for images, targets in val_loader:
                images, targets = images.to(device), targets.to(device)
                outputs = model(images)
                loss = criterion(outputs, targets)
                val_loss += loss.item()
                val_batches += 1
        
        avg_val_loss = val_loss / val_batches
        history['val_loss'].append(avg_val_loss)
        
        epoch_time = time.time() - epoch_start
        
        # –í–∏–≤—ñ–¥ –ø—Ä–æ–≥—Ä–µ—Å—É –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
        print(f"Epoch {epoch+1:2d}/{epochs} | "
              f"Train: {avg_train_loss:.4f} | "
              f"Val: {avg_val_loss:.4f} | "
              f"Time: {epoch_time:.1f}s")
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            history['best_epoch'] = epoch
    
    total_time = time.time() - start_time
    
    print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –ù–∞–π–∫—Ä–∞—â–∏–π Val Loss: {best_val_loss:.4f} (epoch {history['best_epoch']+1})")
    print(f"‚è±Ô∏è  –ß–∞—Å: {total_time/60:.2f} —Ö–≤–∏–ª–∏–Ω")
    
    return best_val_loss, history

def compute_spearman_correlation(proxy_scores, final_losses):
    """–ü–æ—Ä–∞—Ö—É–≤–∞—Ç–∏ –∫–æ—Ä–µ–ª—è—Ü—ñ—é Spearman –º—ñ–∂ Composite Score —Ç–∞ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–º loss"""
    
    # –†–∞–Ω–∂—É–≤–∞–Ω–Ω—è
    proxy_ranks = np.argsort(np.argsort(proxy_scores))
    final_ranks = np.argsort(np.argsort(final_losses))
    
    # Spearman correlation
    rho, pvalue = spearmanr(proxy_scores, final_losses)
    
    # Rank stability (—á–∏ —Å–ø—ñ–≤–ø–∞–¥–∞—é—Ç—å —Ä–∞–Ω–≥–∏)
    rank_match = (proxy_ranks == final_ranks).sum() / len(proxy_ranks) * 100
    
    return {
        'rho': rho,
        'pvalue': pvalue,
        'proxy_ranks': proxy_ranks.tolist(),
        'final_ranks': final_ranks.tolist(),
        'rank_stability': rank_match
    }

# ============================================
# Optuna objective function
# ============================================

def objective(trial):
    """–§—É–Ω–∫—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –¥–ª—è Optuna"""
    
    # 1. –°–¢–†–£–ö–¢–£–†–ù–ò–ô –°–ò–ù–¢–ï–ó
    num_blocks = trial.suggest_int('num_blocks', 2, 5)
    
    filters_list = []
    kernel_sizes = []
    for i in range(num_blocks):
        filters_list.append(trial.suggest_categorical(f'filters_{i}', [16, 32, 64, 128]))
        kernel_sizes.append(trial.suggest_categorical(f'kernel_{i}', [3, 5]))
    
    fc_size = trial.suggest_categorical('fc_size', [64, 128, 256])
    dropout = trial.suggest_categorical('dropout', [0.3, 0.5, 0.7])
    activation = trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'gelu'])
    
    # 2. –ü–ê–†–ê–ú–ï–¢–†–ò–ß–ù–ò–ô –°–ò–ù–¢–ï–ó
    lr = trial.suggest_categorical('lr', [0.0001, 0.001, 0.01])
    batch_size = trial.suggest_categorical('batch_size', BATCH_SIZE_OPTIONS)
    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'adamw', 'sgd'])
    weight_decay = trial.suggest_categorical('weight_decay', [0, 1e-5, 1e-4, 1e-3])
    
    # 3. –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
    model = DynamicDetector(
        num_blocks=num_blocks,
        filters_list=filters_list,
        kernel_sizes=kernel_sizes,
        fc_size=fc_size,
        dropout=dropout,
        activation=activation
    ).to(device)
    
    # 4. –°—Ç–≤–æ—Ä–∏—Ç–∏ optimizer
    if optimizer_name == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)
    else:
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # 5. Dataset
    train_dataset = SimpleDataset("dataset/train/images", "dataset/train/labels")
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø–æ–≤–Ω–∏–π val dataset –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è subset
    val_dataset_full = SimpleDataset("dataset/val/images", "dataset/val/labels", max_samples=-1)
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ validation subset –¥–ª—è Composite Score
    if USE_COMPOSITE_PROXY:
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–∏–π val subset
        val_subset_file = 'results/val_subset_idx.npy'
        val_size = min(VAL_SUBSET, len(val_dataset_full))
        
        if Path(val_subset_file).exists():
            val_indices = np.load(val_subset_file)
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —â–æ —ñ–Ω–¥–µ–∫—Å–∏ –≤ –º–µ–∂–∞—Ö
            val_indices = val_indices[val_indices < len(val_dataset_full)]
            if len(val_indices) < val_size:
                # –ü–µ—Ä–µ–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —è–∫—â–æ –Ω–µ –≤–∏—Å—Ç–∞—á–∞—î
                np.random.seed(SEED)
                val_indices = np.random.choice(len(val_dataset_full), val_size, replace=False)
                np.save(val_subset_file, val_indices)
        else:
            np.random.seed(SEED)
            val_indices = np.random.choice(len(val_dataset_full), val_size, replace=False)
            np.save(val_subset_file, val_indices)
        
        val_dataset = torch.utils.data.Subset(val_dataset_full, val_indices)
    else:
        val_dataset = val_dataset_full
    
    # num_workers=0 –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ MPS
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    # 6. –ù–∞–≤—á–∞–Ω–Ω—è
    try:
        val_loss, metrics = train_trial(model, train_loader, val_loader, device, optimizer, 
                                       epochs=EPOCHS_PER_TRIAL, use_composite=USE_COMPOSITE_PROXY)
    except Exception as e:
        print(f"‚ö†Ô∏è  Trial failed: {e}")
        return float('inf')
    
    # 7. –û–±—á–∏—Å–ª–∏—Ç–∏ objective (Composite Score –∞–±–æ –ø—Ä–æ—Å—Ç–∏–π val_loss)
    if USE_COMPOSITE_PROXY and metrics is not None and proxy_stats is not None:
        # –û–±—á–∏—Å–ª–∏—Ç–∏ Composite Score
        proxy_value, objective_type = compute_composite_score(metrics, proxy_stats)
        
        # –û–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É warmup
        if not proxy_stats.ready():
            proxy_stats.update(metrics)
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ user attributes
        for key, value in metrics.items():
            trial.set_user_attr(key, value)
        trial.set_user_attr('proxy_value', proxy_value)
        trial.set_user_attr('objective_type', objective_type)
        
        # –õ–æ–≥—É–≤–∞—Ç–∏ –≤ CSV
        proxy_stats.log_trial(trial.number, metrics, proxy_value, objective_type)
        
        return proxy_value
    
    return val_loss

# ============================================
# –ó–∞–ø—É—Å–∫ —Å–∏–Ω—Ç–µ–∑—É
# ============================================

if __name__ == "__main__":
    # ============================================
    # –ó–∞–ø—É—Å–∫ –ª–æ–≥—É–≤–∞–Ω–Ω—è
    # ============================================
    
    # –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–∏—Ç–∏ stdout –≤ TeeLogger
    tee_logger.start()
    sys.stdout = tee_logger
    
    log_print(f"üöÄ –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–ø—É—â–µ–Ω–æ: {EXPERIMENT_START_TIME.strftime('%Y-%m-%d %H:%M:%S UTC')}")
    log_print(f"üìù –õ–æ–≥–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è —É: {LOG_FILENAME}")
    
    # ============================================
    # –í–∏–≤—ñ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    # ============================================
    
    print("\n" + "="*60)
    print("–£–ù–Ü–í–ï–†–°–ê–õ–¨–ù–ò–ô –°–¢–†–£–ö–¢–£–†–ù–û-–ü–ê–†–ê–ú–ï–¢–†–ò–ß–ù–ò–ô –°–ò–ù–¢–ï–ó")
    print("="*60)
    
    # –í–∏–≤—ñ–¥ —Ä–µ–∂–∏–º—É —Ä–æ–±–æ—Ç–∏
    mode_emoji = "üöÄ" if FULL_RUN_MODE else "‚ö°"
    mode_name = "–ü–û–í–ù–ò–ô –ü–†–û–ì–û–ù" if FULL_RUN_MODE else "–®–í–ò–î–ö–ò–ô –¢–ï–°–¢"
    platform = "Google Colab" if IS_COLAB else "–õ–æ–∫–∞–ª—å–Ω–æ"
    print(f"\n{mode_emoji} –†–ï–ñ–ò–ú: {mode_name} ({platform})")
    
    print(f"\n‚öôÔ∏è  –ü–ê–†–ê–ú–ï–¢–†–ò:")
    print(f"   Trials: {N_TRIALS}")
    print(f"   Timeout: {TIMEOUT}s ({TIMEOUT/60:.0f} min)")
    print(f"   Max samples: {MAX_SAMPLES}")
    print(f"   Image size: {IMG_SIZE}√ó{IMG_SIZE}")
    print(f"   Epochs per trial: {EPOCHS_PER_TRIAL}")
    if FULL_PIPELINE:
        print(f"   Full training epochs: {FULL_EPOCHS}")
    
    # –í–∏–≤—ñ–¥ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π
    if gpu_memory:
        print(f"\n‚úÖ GPU: {gpu_name}")
        print(f"   VRAM: {gpu_memory:.1f} GB")
    else:
        if device.type == "mps":
            print(f"\n‚úÖ GPU: {gpu_name}")
        else:
            print(f"\n‚ö†Ô∏è  {gpu_name}")
    print(f"   Device: {device}")
    print(f"   Batch sizes: {BATCH_SIZE_OPTIONS}")
    
    print("\n" + "="*60)
    print("–ó–ê–ü–£–°–ö –°–ò–ù–¢–ï–ó–£")
    print("="*60)
    
    if USE_COMPOSITE_PROXY:
        print(f"\nüìä Phase 1: Warmup –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ—è (trials 1-{N_WARMUP})")
        print(f"   ‚Üí –ó–±—ñ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è robust z-–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
        print(f"\nüß† Phase 2: Bayesian Optimization (trials {N_WARMUP+1}-{N_TRIALS})")
        print(f"   ‚Üí –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∑–∞ Composite Score")
    
    # ============================================
    # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ Study –∑ checkpoint (—è–∫—â–æ —î)
    # ============================================
    
    # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ checkpoint –∑ Drive (–Ω–∞ Colab)
    if IS_COLAB:
        try:
            from google.colab import drive
            import shutil
            
            # Drive –≤–∂–µ –∑–º–æ–Ω—Ç–æ–≤–∞–Ω–æ —Ä–∞–Ω—ñ—à–µ
            drive_checkpoint = '/content/drive/MyDrive/Studying/Experiments/Composite_score_nas/checkpoint'
            
            if Path(drive_checkpoint).exists():
                # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î —Ñ–∞–π–ª–∏ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ
                files = list(Path(drive_checkpoint).glob('*'))
                
                if files:
                    log_print(f"üîÑ –ó–Ω–∞–π–¥–µ–Ω–æ checkpoint –Ω–∞ Google Drive! –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è {len(files)} —Ñ–∞–π–ª—ñ–≤...")
                    
                    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—É –ø–∞–ø–∫—É results
                    Path('results').mkdir(exist_ok=True)
                    
                    # –°–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –≤—Å—ñ —Ñ–∞–π–ª–∏ checkpoint
                    copied_count = 0
                    for file in files:
                        if file.is_file():
                            shutil.copy2(file, 'results/')
                            copied_count += 1
                    
                    log_print(f"   ‚úÖ Checkpoint —Å–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ –∑ Drive ({copied_count} —Ñ–∞–π–ª—ñ–≤)")
                else:
                    log_print(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è checkpoint —ñ—Å–Ω—É—î, –∞–ª–µ –ü–û–†–û–ñ–ù–Ø! –Ü–≥–Ω–æ—Ä—É—î–º–æ.")
            else:
                log_print("‚úÖ Checkpoint –Ω–∞ Drive –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - —á–∏—Å—Ç–∏–π —Å—Ç–∞—Ä—Ç!")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ checkpoint –∑ Drive: {e}")
    
    study_checkpoint = Path('results/optuna_study.pkl')
    study_resumed = False
    
    if study_checkpoint.exists():
        try:
            print("\nüîÑ –ó–Ω–∞–π–¥–µ–Ω–æ Optuna checkpoint! –°–ø—Ä–æ–±–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è...")
            import pickle
            
            with open(study_checkpoint, 'rb') as f:
                study = pickle.load(f)
            
            study_resumed = True
            print(f"   ‚úÖ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ {len(study.trials)} trials")
            print(f"   –ü—Ä–æ–¥–æ–≤–∂–∏–º–æ –∑ trial #{len(study.trials) + 1}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ study: {e}")
            study_resumed = False
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π study —è–∫—â–æ –Ω–µ –≤—ñ–¥–Ω–æ–≤–∏–ª–∏
    if not study_resumed:
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=SEED)
        )
    
    # Callback –¥–ª—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–≤–æ–¥—É
    def progress_callback(study, trial):
        if trial.state == optuna.trial.TrialState.COMPLETE:
            best_trial = study.best_trial
            phase = "Warmup" if trial.number < N_WARMUP else "Composite Score"
            status = "üî•" if trial.value == best_trial.value else "‚úì"
            
            log_print(f"{status} Trial {trial.number + 1}/{N_TRIALS} [{phase}]:")
            print(f"   –ü–æ—Ç–æ—á–Ω–∏–π Score: {trial.value:.4f}")
            print(f"   –ù–∞–π–∫—Ä–∞—â–∏–π Score: {best_trial.value:.4f} (Trial #{best_trial.number + 1})")
            print(f"   –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: {trial.params['num_blocks']} –±–ª–æ–∫—ñ–≤, "
                  f"Act={trial.params['activation']}, Opt={trial.params['optimizer']}")
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é
    start_time = time.time()
    
    # –Ø–∫—â–æ –≤—ñ–¥–Ω–æ–≤–∏–ª–∏ - –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –º–µ–Ω—à–µ trials
    remaining_trials = N_TRIALS - len(study.trials) if study_resumed else N_TRIALS
    
    if remaining_trials > 0:
        # –í–∏–º–∫–Ω—É—Ç–∏ verbose –ª–æ–≥–∏ Optuna
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        
        study.optimize(
            objective,
            n_trials=remaining_trials,
            timeout=TIMEOUT,
            callbacks=[progress_callback],
            show_progress_bar=False  # –í–∏–º–∫–Ω—É—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä
        )
    else:
        print(f"\n‚è≠Ô∏è  –§–∞–∑—É —Å–∏–Ω—Ç–µ–∑—É –ø—Ä–æ–ø—É—â–µ–Ω–æ - –≤–∂–µ —î {len(study.trials)} trials")
        print(f"   –ü–µ—Ä–µ—Ö–æ–¥–∂—É –æ–¥—Ä–∞–∑—É –¥–æ –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è")
    
    synthesis_time = time.time() - start_time
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ study checkpoint
    try:
        import pickle
        Path('results').mkdir(exist_ok=True)
        with open('results/optuna_study.pkl', 'wb') as f:
            pickle.dump(study, f)
        print(f"\nüíæ Optuna study –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ study: {e}")
    
    # ============================================
    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
    # ============================================
    
    print("\n" + "="*60)
    if remaining_trials == 0:
        print("–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò –°–ò–ù–¢–ï–ó–£")
    else:
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–ò –°–ò–ù–¢–ï–ó–£")
    print("="*60)
        
    if synthesis_time > 0:
        print(f"\n‚è±Ô∏è  –ß–∞—Å —Å–∏–Ω—Ç–µ–∑—É: {synthesis_time:.2f}s ({synthesis_time/60:.2f} min)")
    print(f"üîç –ü–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä: {len(study.trials)}")
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–∏—Ö: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
    
    print(f"\nüèÜ –ù–ê–ô–ö–†–ê–©–ê –ê–†–•–Ü–¢–ï–ö–¢–£–†–ê:")
    score_name = "Composite Score" if USE_COMPOSITE_PROXY and proxy_stats.ready() else "Val Loss"
    print(f"   {score_name}: {study.best_value:.4f} (Trial {study.best_trial.number + 1})")
    print(f"   {'   ‚Üì –º–µ–Ω—à–µ = –∫—Ä–∞—â–µ (–º—ñ–Ω—ñ–º—ñ–∑—É—î–º–æ)' if study.best_value < 0 else ''}")
    
    print(f"\nüìê –°—Ç—Ä—É–∫—Ç—É—Ä–∞:")
    best_params = study.best_params
    num_blocks = best_params['num_blocks']
    print(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å conv –±–ª–æ–∫—ñ–≤: {num_blocks}")
    for i in range(num_blocks):
        print(f"   Block {i+1}: {best_params[f'filters_{i}']} —Ñ—ñ–ª—å—Ç—Ä—ñ–≤, kernel {best_params[f'kernel_{i}']}√ó{best_params[f'kernel_{i}']}")
    print(f"   FC layer: {best_params['fc_size']}")
    print(f"   Dropout: {best_params['dropout']}")
    print(f"   Activation: {best_params['activation']}")
    
    print(f"\n‚öôÔ∏è  –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏:")
    print(f"   Optimizer: {best_params['optimizer']}")
    print(f"   Learning rate: {best_params['lr']}")
    print(f"   Weight decay: {best_params['weight_decay']}")
    print(f"   Batch size: {best_params['batch_size']}")
        
    # ============================================
    # TOP 5
    # ============================================
    
    print("\n" + "="*60)
    print(f"TOP 5 –ê–†–•–Ü–¢–ï–ö–¢–£–† (–∑–∞ {score_name}):")
    print("="*60)
    
    sorted_trials = sorted(study.trials, key=lambda t: t.value if t.value else float('inf'))
    for i, trial in enumerate(sorted_trials[:5]):
        if trial.value:
            indicator = "üî•" if i == 0 else ("‚≠ê" if i < 3 else "‚úì")
            print(f"\n{indicator} #{i+1} Score: {trial.value:.4f} (Trial {trial.number + 1})")
            print(f"    –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: {trial.params['num_blocks']} –±–ª–æ–∫—ñ–≤, "
                  f"FC={trial.params['fc_size']}, "
                  f"{trial.params['activation']}")
            print(f"    –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {trial.params['optimizer']}, "
                  f"LR={trial.params['lr']}, "
                  f"BS={trial.params['batch_size']}")
        
    
    # ============================================
    # –ü–æ–≤–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –í–°–Ü–• –º–æ–¥–µ–ª–µ–π (—è–∫—â–æ FULL_PIPELINE=True)
    # ============================================
    
    if FULL_PIPELINE:
        print("\n" + "="*60)
        print(f"–ü–û–í–ù–ï –¢–†–ï–ù–£–í–ê–ù–ù–Ø –í–°–Ü–• {len(study.trials)} –ú–û–î–ï–õ–ï–ô")
        print("="*60)
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ batch size –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        full_batch_size = FULL_BATCH_SIZE
        if full_batch_size is None:
            full_batch_size = 32 if device.type == "cuda" else 16
        
        print(f"\n‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è:")
        print(f"   Epochs: {FULL_EPOCHS}")
        print(f"   Batch size: {full_batch_size}")
        print(f"   Device: {device}")
        
        # ============================================
        # –°–ø—Ä–æ–±–∞ –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ –∑ checkpoint (—è–∫—â–æ —î)
        # ============================================
        
        checkpoint_path = Path('results/checkpoint.json')
        resumed = False
        full_training_results = []
        
        if checkpoint_path.exists():
            try:
                print("\nüîÑ –ó–Ω–∞–π–¥–µ–Ω–æ checkpoint! –°–ø—Ä–æ–±–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è...")
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —Ü–µ —Ç–æ–π —Å–∞–º–∏–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
                if (checkpoint_data['synthesis']['n_trials'] == N_TRIALS and
                    checkpoint_data['synthesis']['seed'] == SEED):
                    
                    full_training_results = checkpoint_data['training']['results']
                    resumed = True
                    print(f"   ‚úÖ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ {len(full_training_results)} –º–æ–¥–µ–ª–µ–π")
                    print(f"   –ü—Ä–æ–¥–æ–≤–∂–∏–º–æ –∑ –º–æ–¥–µ–ª—ñ #{len(full_training_results) + 1}")
                else:
                    print("   ‚ö†Ô∏è  Checkpoint –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É - —ñ–≥–Ω–æ—Ä—É—î–º–æ")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ checkpoint: {e}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        print(f"\nüì¶ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
        full_train_dataset = SimpleDataset("dataset/train/images", "dataset/train/labels", img_size=IMG_SIZE, max_samples=FULL_MAX_SAMPLES)
        full_val_dataset = SimpleDataset("dataset/val/images", "dataset/val/labels", img_size=IMG_SIZE, max_samples=-1)
        print(f"   Train: {len(full_train_dataset)} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
        print(f"   Val: {len(full_val_dataset)} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
        
        # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –≤—Å—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (–≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ –ø–æ Composite Score)
        all_models_to_train = []
        for trial in study.trials:
            if trial.state == optuna.trial.TrialState.COMPLETE:
                all_models_to_train.append({
                    'trial_number': trial.number,
                    'proxy_value': trial.value,
                    'params': trial.params,
                    'user_attrs': trial.user_attrs
                })
        
        # –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ Composite Score (ascending - –∫—Ä–∞—â—ñ –º–∞—é—Ç—å –º–µ–Ω—à–∏–π score)
        all_models_to_train.sort(key=lambda x: x['proxy_value'])
        
        print(f"\nüìã –ë—É–¥–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ {len(all_models_to_train)} –º–æ–¥–µ–ª–µ–π")
        print(f"   Composite Score range: {all_models_to_train[0]['proxy_value']:.4f} (–Ω–∞–π–∫—Ä–∞—â–∏–π) ‚Üí {all_models_to_train[-1]['proxy_value']:.4f} (–Ω–∞–π–≥—ñ—Ä—à–∏–π)")
        
        # –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ –ö–û–ñ–ù–£ –º–æ–¥–µ–ª—å
        for idx, model_info in enumerate(all_models_to_train, 1):
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ —è–∫—â–æ –≤–∂–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ
            if resumed and idx <= len(full_training_results):
                continue
            
            params = model_info['params']
            
            print(f"\n{'='*60}")
            log_print(f"–ú–û–î–ï–õ–¨ #{idx}/{len(all_models_to_train)} (Trial {model_info['trial_number']})")
            print(f"{'='*60}")
            log_print(f"   Composite Score (—Å–∏–Ω—Ç–µ–∑): {model_info['proxy_value']:.4f}")
            print(f"   Proxy rank: #{idx}")
            print(f"   –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {params['num_blocks']} –±–ª–æ–∫—ñ–≤")
            
            # –í–∏—Ç—è–≥—Ç–∏ filters —Ç–∞ kernels
            filters_list = [params[f'filters_{i}'] for i in range(params['num_blocks'])]
            kernel_sizes = [params[f'kernel_{i}'] for i in range(params['num_blocks'])]
            
            print(f"   –§—ñ–ª—å—Ç—Ä–∏: {filters_list}")
            print(f"   Activation: {params['activation']}")
            
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
            model = DynamicDetector(
                num_blocks=params['num_blocks'],
                filters_list=filters_list,
                kernel_sizes=kernel_sizes,
                fc_size=params['fc_size'],
                dropout=params['dropout'],
                activation=params['activation']
            ).to(device)
            
            # DataLoaders
            train_loader = DataLoader(
                full_train_dataset, 
                batch_size=full_batch_size,
                shuffle=True, 
                num_workers=0
            )
            val_loader = DataLoader(
                full_val_dataset, 
                batch_size=full_batch_size,
                shuffle=False, 
                num_workers=0
            )
            
            # Optimizer config
            optimizer_config = {
                'optimizer': params['optimizer'],
                'lr': params['lr'],
                'weight_decay': params['weight_decay']
            }
            
            # –ü–æ–≤–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
            model_name = f"model_trial{model_info['trial_number']}_rank{idx}_seed{SEED}"
            final_val_loss, history = train_model_full(
                model, 
                train_loader, 
                val_loader, 
                device, 
                optimizer_config=optimizer_config,
                epochs=FULL_EPOCHS,
                model_name=model_name
            )
            
            # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            full_training_results.append({
                'trial_number': model_info['trial_number'],
                'proxy_rank': idx,
                'proxy_score': model_info['proxy_value'],
                'final_val_loss': final_val_loss,
                'improvement': model_info['proxy_value'] - final_val_loss,
                'history': history,
                'params': params
            })
            
            # ============================================
            # Checkpoint –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è (–ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ)
            # ============================================
            try:
                print(f"\nüíæ Checkpoint: –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–º—ñ–∂–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")
                
                # –ó–±–µ—Ä–µ–≥—Ç–∏ –ø—Ä–æ–º—ñ–∂–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
                checkpoint_results = {
                    'synthesis': {
                        'n_trials': N_TRIALS,
                        'n_completed': len(all_models_to_train),
                        'use_composite': USE_COMPOSITE_PROXY,
                        'seed': SEED
                    },
                    'training': {
                        'epochs': FULL_EPOCHS,
                        'batch_size': full_batch_size,
                        'n_models_trained': len(full_training_results),
                        'results': full_training_results,
                        'checkpoint': f'Model {idx}/{len(all_models_to_train)}'
                    }
                }
                
                # –ó–±–µ—Ä–µ–≥—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ (–∑–∞–≤–∂–¥–∏)
                with open('results/checkpoint.json', 'w', encoding='utf-8') as f:
                    json.dump(checkpoint_results, f, indent=2, ensure_ascii=False)
                
                # –ó–±–µ—Ä–µ–≥—Ç–∏ Optuna study (–∑–∞–≤–∂–¥–∏)
                import pickle
                with open('results/optuna_study.pkl', 'wb') as f:
                    pickle.dump(study, f)
                
                print(f"   ‚úÖ Checkpoint –∑–±–µ—Ä–µ–∂–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ (–º–æ–¥–µ–ª—å {idx}/{len(all_models_to_train)})")
                
                # –î–æ–¥–∞—Ç–∫–æ–≤–æ –Ω–∞ Drive (—Ç—ñ–ª—å–∫–∏ Colab)
                if IS_COLAB:
                    from google.colab import drive
                    import shutil
                    
                    drive.mount('/content/drive', force_remount=False)
                    drive_results_dir = '/content/drive/MyDrive/Studying/Experiments/Composite_score_nas'
                    Path(drive_results_dir).mkdir(parents=True, exist_ok=True)
                    
                    shutil.copytree('results', f'{drive_results_dir}/checkpoint', dirs_exist_ok=True)
                    print(f"   ‚úÖ Checkpoint —Ç–∞–∫–æ–∂ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –Ω–∞ Drive")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ checkpoint: {e}")
        
        # ============================================
        # –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó Composite Score ‚Üí Final Loss
        # ============================================
        
        print("\n" + "="*60)
        print("–ê–ù–ê–õ–Ü–ó –ö–û–†–ï–õ–Ø–¶–Ü–á COMPOSITE SCORE ‚Üí FINAL LOSS")
        print("="*60)
        
        proxy_scores = [r['proxy_score'] for r in full_training_results]
        final_losses = [r['final_val_loss'] for r in full_training_results]
        
        correlation_stats = compute_spearman_correlation(proxy_scores, final_losses)
        
        print(f"\nüìä Spearman –∫–æ—Ä–µ–ª—è—Ü—ñ—è:")
        print(f"   œÅ = {correlation_stats['rho']:.4f}")
        print(f"   p-value = {correlation_stats['pvalue']:.4f}")
        if correlation_stats['pvalue'] < 0.05:
            print(f"   ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ –∑–Ω–∞—á—É—â–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è!")
        print(f"   Rank stability: {correlation_stats['rank_stability']:.1f}%")
        
        # –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ —Ñ—ñ–Ω–∞–ª—å–Ω–æ–º—É loss –¥–ª—è –≤–∏–≤–æ–¥—É —Ç–æ–ø-10
        sorted_by_final = sorted(full_training_results, key=lambda x: x['final_val_loss'])
        
        print(f"\nüìà –¢–û–ü-10 –∑–∞ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–º loss:")
        for i, result in enumerate(sorted_by_final[:10], 1):
            final_rank = i
            proxy_rank = result['proxy_rank']
            rank_diff = abs(proxy_rank - final_rank)
            rank_indicator = "‚úÖ" if rank_diff <= 5 else ("‚ö†Ô∏è" if rank_diff <= 10 else "‚ùå")
            
            print(f"\n#{i} (Trial {result['trial_number']}):")
            print(f"   Final Loss: {result['final_val_loss']:.4f}")
            print(f"   Composite Score: {result['proxy_score']:.4f} (–±—É–≤ rank #{proxy_rank}) {rank_indicator}")
            print(f"   Rank diff: {rank_diff}")
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ –ø–æ–≤–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        results_filename = 'results/synthesis_results.json'
        full_results = {
            'synthesis': {
                'n_trials': N_TRIALS,
                'n_completed': len(all_models_to_train),
                'use_composite': USE_COMPOSITE_PROXY,
                'seed': SEED
            },
            'training': {
                'epochs': FULL_EPOCHS,
                'batch_size': full_batch_size,
                'n_models_trained': len(full_training_results),
                'results': full_training_results
            },
            'correlation': correlation_stats
        }
        
        with open(results_filename, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –ü–æ–≤–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {results_filename}")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–∞ Google Drive (—è–∫—â–æ Colab)
        try:
            from google.colab import drive, files
            import shutil
            
            # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞ Google Drive
            try:
                drive.mount('/content/drive', force_remount=False)
                drive_results_dir = '/content/drive/MyDrive/Studying/Experiments/Composite_score_nas'
                Path(drive_results_dir).mkdir(parents=True, exist_ok=True)
                
                # –°–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –≤—Å—é –ø–∞–ø–∫—É results
                shutil.copytree('results', f'{drive_results_dir}/results_full', dirs_exist_ok=True)
                print(f"\n‚òÅÔ∏è  –ü–æ–≤–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –Ω–∞ Google Drive: {drive_results_dir}/results_full/")
            except Exception as e:
                print(f"\n‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞ Drive: {e}")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–ª—é—á–æ–≤—ñ —Ñ–∞–π–ª–∏
            print("\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –∫–ª—é—á–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏...")
            try:
                files.download('results/synthesis_results.json')
                files.download('results/trials_proxy_metrics.csv')
                print("‚úÖ –ö–ª—é—á–æ–≤—ñ —Ñ–∞–π–ª–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!")
            except Exception as e:
                print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
        except ImportError:
            pass  # –ù–µ –Ω–∞ Colab
    
    elif not FULL_PIPELINE:
        # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∏–Ω—Ç–µ–∑—É (–±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è)
        results_filename = 'results/synthesis_results.json'
        synthesis_only_results = {
            'synthesis': {
                'n_trials': N_TRIALS,
                'n_completed': len(study.trials),
                'use_composite': USE_COMPOSITE_PROXY,
                'seed': SEED
            },
            'note': '–¢—ñ–ª—å–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∏–Ω—Ç–µ–∑—É. –î–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å FULL_PIPELINE=True'
        }
        
        with open(results_filename, 'w', encoding='utf-8') as f:
            json.dump(synthesis_only_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∏–Ω—Ç–µ–∑—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {results_filename}")
        print(f"\nüìÅ –î–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å FULL_PIPELINE=True —É —Å–∫—Ä–∏–ø—Ç—ñ")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –Ω–∞ Colab
        try:
            from google.colab import drive, files
            import shutil
            
            # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞ Google Drive
            try:
                drive.mount('/content/drive', force_remount=False)
                drive_results_dir = '/content/drive/MyDrive/Studying/Experiments/Composite_score_nas'
                Path(drive_results_dir).mkdir(parents=True, exist_ok=True)
                
                # –°–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –≤—Å—é –ø–∞–ø–∫—É results (—Ç—ñ–ª—å–∫–∏ —Å–∏–Ω—Ç–µ–∑, –±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è)
                shutil.copytree('results', f'{drive_results_dir}/results_only_synthesis', dirs_exist_ok=True)
                print(f"\n‚òÅÔ∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∏–Ω—Ç–µ–∑—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ –Ω–∞ Google Drive: {drive_results_dir}/results_only_synthesis/")
            except Exception as e:
                print(f"\n‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞ Drive: {e}")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ synthesis_results.json
            print("\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é synthesis_results.json...")
            files.download('results/synthesis_results.json')
            print("‚úÖ –§–∞–π–ª –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!")
        except ImportError:
            pass  # –ù–µ –Ω–∞ Colab
    
    # –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è
    experiment_end_time = datetime.now(timezone.utc)
    duration = (experiment_end_time - EXPERIMENT_START_TIME).total_seconds()
    
    print("\n" + "="*60)
    log_print("‚úÖ –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù–û!")
    log_print(f"‚è±Ô∏è  –¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å: {duration/3600:.2f} –≥–æ–¥–∏–Ω ({duration/60:.1f} —Ö–≤–∏–ª–∏–Ω)")
    log_print(f"üìù –ü–æ–≤–Ω–∏–π –ª–æ–≥ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É: {LOG_FILENAME}")
    print("="*60)
    
    # –ó–∞–∫—Ä–∏—Ç–∏ –ª–æ–≥ —Ñ–∞–π–ª
    sys.stdout = tee_logger.terminal
    tee_logger.close()