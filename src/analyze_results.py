"""
–ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É
–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è DSS vs baseline, –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

–ê–≤—Ç–æ—Ä: –ê–Ω–∞—Ç–æ–ª—ñ–π –ö–æ—Ç
–î–∞—Ç–∞: 2026-01-24
"""

import json
import pickle
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr

from synthesis_universal import RESULTS_DIR, CHECKPOINT_FILE


def load_study():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Optuna study"""
    if not CHECKPOINT_FILE.exists():
        raise FileNotFoundError(f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª study: {CHECKPOINT_FILE}")
    
    with open(CHECKPOINT_FILE, 'rb') as f:
        return pickle.load(f)


def analyze_convergence(study):
    """–ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó"""
    
    trials = study.trials
    trial_numbers = [t.number for t in trials]
    values = [t.value for t in trials]
    
    # Best value so far
    best_values = []
    current_best = float('inf')
    for v in values:
        current_best = min(current_best, v)
        best_values.append(current_best)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: Trial values
    axes[0].plot(trial_numbers, values, 'o-', alpha=0.6, label='Trial value')
    axes[0].plot(trial_numbers, best_values, 'r-', linewidth=2, label='Best so far')
    axes[0].axvline(x=10, color='green', linestyle='--', label='Warmup end')
    axes[0].set_xlabel('Trial number')
    axes[0].set_ylabel('Objective value')
    axes[0].set_title('Convergence Plot')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Plot 2: Improvement over warmup
    if len(values) > 10:
        warmup_best = min(values[:10])
        improvements = [(warmup_best - v) / abs(warmup_best) * 100 for v in values[10:]]
        axes[1].plot(range(11, len(values)+1), improvements, 'o-')
        axes[1].axhline(y=0, color='red', linestyle='--')
        axes[1].set_xlabel('Trial number')
        axes[1].set_ylabel('Improvement over warmup (%)')
        axes[1].set_title('DSS Improvement')
        axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(RESULTS_DIR / 'convergence.png', dpi=150)
    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {RESULTS_DIR / 'convergence.png'}")


def analyze_hyperparameters(study):
    """–ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
    
    # –¢–æ–ø-10 trials
    trials_sorted = sorted(study.trials, key=lambda t: t.value)
    top_trials = trials_sorted[:10]
    
    # –ó–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    params_analysis = {}
    
    for trial in top_trials:
        for key, value in trial.params.items():
            if key not in params_analysis:
                params_analysis[key] = []
            params_analysis[key].append(value)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —á–∞—Å—Ç–æ—Ç–∏
    print(f"\n{'='*60}")
    print("üìä –ê–Ω–∞–ª—ñ–∑ —Ç–æ–ø-10 –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä")
    print(f"{'='*60}")
    
    for param, values in params_analysis.items():
        if 'filter_' in param or 'kernel_' in param:
            continue  # Skip individual layer params
        
        if isinstance(values[0], (int, float)):
            print(f"\n{param}:")
            print(f"   Mean: {np.mean(values):.3f}")
            print(f"   Std: {np.std(values):.3f}")
        else:
            from collections import Counter
            freq = Counter(values)
            print(f"\n{param}:")
            for val, count in freq.most_common():
                percentage = (count / len(values)) * 100
                print(f"   {val}: {count}/10 ({percentage:.0f}%)")


def analyze_architecture_patterns(study):
    """–ê–Ω–∞–ª—ñ–∑ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤"""
    
    trials_sorted = sorted(study.trials, key=lambda t: t.value)
    top_trials = trials_sorted[:10]
    
    print(f"\n{'='*60}")
    print("üèóÔ∏è  –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ (—Ç–æ–ø-10)")
    print(f"{'='*60}")
    
    # Depth analysis
    depths = [t.params['n_blocks'] for t in top_trials]
    print(f"\nüìê –ì–ª–∏–±–∏–Ω–∞ (n_blocks):")
    from collections import Counter
    depth_freq = Counter(depths)
    for depth, count in sorted(depth_freq.items()):
        print(f"   {depth} blocks: {count}/10")
    
    # Filter patterns
    print(f"\nüî¢ –ü–∞—Ç—Ç–µ—Ä–Ω–∏ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤:")
    for i, trial in enumerate(top_trials[:5], 1):
        n_blocks = trial.params['n_blocks']
        filters = [trial.params[f'filter_{j}'] for j in range(n_blocks)]
        kernels = [trial.params[f'kernel_{j}'] for j in range(n_blocks)]
        print(f"   #{i}: filters={filters}, kernels={kernels}")


def compare_with_baseline(study):
    """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è DSS –∑ baseline (validation loss)"""
    
    # Warmup trials (baseline)
    warmup_trials = study.trials[:10]
    warmup_best = min(warmup_trials, key=lambda t: t.value)
    
    # DSS trials
    dss_trials = study.trials[10:]
    if len(dss_trials) > 0:
        dss_best = min(dss_trials, key=lambda t: t.value)
        
        print(f"\n{'='*60}")
        print("‚öñÔ∏è  Baseline vs DSS")
        print(f"{'='*60}")
        print(f"\nüîµ Baseline (validation loss):")
        print(f"   Best trial: #{warmup_best.number}")
        print(f"   Value: {warmup_best.value:.4f}")
        
        print(f"\nüü¢ DSS (stability-aware):")
        print(f"   Best trial: #{dss_best.number}")
        print(f"   Value: {dss_best.value:.4f}")
        
        improvement = ((warmup_best.value - dss_best.value) / abs(warmup_best.value)) * 100
        print(f"\nüìà –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {improvement:+.2f}%")


def save_analysis_report(study):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É"""
    
    trials_sorted = sorted(study.trials, key=lambda t: t.value)
    
    report = {
        'summary': {
            'total_trials': len(study.trials),
            'best_value': study.best_value,
            'best_trial': study.best_trial.number
        },
        'top_10': [
            {
                'rank': i,
                'trial_number': t.number,
                'value': t.value,
                'params': t.params
            }
            for i, t in enumerate(trials_sorted[:10], 1)
        ]
    }
    
    report_file = RESULTS_DIR / 'analysis_report.json'
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\n‚úÖ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_file}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –∞–Ω–∞–ª—ñ–∑—É"""
    
    print("üî¨ –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É")
    print(f"{'='*60}\n")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    study = load_study()
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ study –∑ {len(study.trials)} trials")
    
    # –ê–Ω–∞–ª—ñ–∑–∏
    analyze_convergence(study)
    analyze_hyperparameters(study)
    analyze_architecture_patterns(study)
    compare_with_baseline(study)
    save_analysis_report(study)
    
    print(f"\n{'='*60}")
    print("‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É: {RESULTS_DIR}")


if __name__ == '__main__':
    main()
