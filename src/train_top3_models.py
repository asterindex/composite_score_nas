"""
–ü–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–æ–ø-3 –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä –ø—ñ—Å–ª—è —Å–∏–Ω—Ç–µ–∑—É
–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π –∑ Optuna study

–ê–≤—Ç–æ—Ä: –ê–Ω–∞—Ç–æ–ª—ñ–π –ö–æ—Ç
–î–∞—Ç–∞: 2026-01-24
"""

import os
import json
import pickle
import time
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# –Ü–º–ø–æ—Ä—Ç –∑ synthesis_universal (—Ç–æ–π —Å–∞–º–∏–π –º–æ–¥—É–ª—å src/)
from synthesis_universal import (
    DynamicDetector, get_dataloaders, evaluate, log_print,
    set_seed, DEVICE, DEVICE_NAME, RESULTS_DIR, CHECKPOINT_FILE,
    LOG_FILE, setup_logging
)


# ============================================================================
# –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø –ü–û–í–ù–û–ì–û –ù–ê–í–ß–ê–ù–ù–Ø
# ============================================================================

FULL_EPOCHS = 25
TOP_K = 3
SEED = 42

MODELS_DIR = RESULTS_DIR / 'trained_models'
FINAL_RESULTS_FILE = RESULTS_DIR / 'final_results.json'


# ============================================================================
# –ü–û–í–ù–ï –ù–ê–í–ß–ê–ù–ù–Ø
# ============================================================================

def train_full_model(config: Dict, trial_number: int, epochs: int = FULL_EPOCHS) -> Dict:
    """
    –ü–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è–º –≤—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫
    
    Args:
        config: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ —Ç–∞ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        trial_number: –ù–æ–º–µ—Ä trial –∑ Optuna
        epochs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –Ω–∞–≤—á–∞–Ω–Ω—è
    
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
    """
    
    log_print(f"\n{'='*60}")
    log_print(f"üöÄ –ü–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è Trial #{trial_number}")
    log_print(f"{'='*60}")
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    set_seed(SEED)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
    n_blocks = config['n_blocks']
    model_config = {
        'n_blocks': n_blocks,
        'filter_sizes': [config[f'filter_{i}'] for i in range(n_blocks)],
        'kernel_sizes': [config[f'kernel_{i}'] for i in range(n_blocks)],
        'fc_size': config['fc_size'],
        'dropout': config['dropout'],
        'activation': config['activation']
    }
    
    # –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    optimizer_name = config['optimizer']
    lr = config['lr']
    weight_decay = config['weight_decay']
    batch_size = config['batch_size']
    
    log_print(f"üèóÔ∏è  Architecture: {n_blocks} blocks")
    log_print(f"   Filters: {model_config['filter_sizes']}")
    log_print(f"   Kernels: {model_config['kernel_sizes']}")
    log_print(f"   FC: {model_config['fc_size']}, Dropout: {model_config['dropout']}")
    log_print(f"   Activation: {model_config['activation']}")
    log_print(f"‚öôÔ∏è  Optimizer: {optimizer_name.upper()} (LR={lr}, WD={weight_decay})")
    log_print(f"üì¶ Batch size: {batch_size}, Epochs: {epochs}")
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    model = DynamicDetector(model_config).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    
    if optimizer_name == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)
    
    # Scheduler (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    train_loader, val_loader = get_dataloaders(batch_size)
    
    # –Ü—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
    history = {
        'train_loss': [],
        'val_loss': [],
        'best_val_loss': float('inf'),
        'best_epoch': 0
    }
    
    start_time = time.time()
    
    # –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è
    for epoch in range(epochs):
        epoch_start = time.time()
        
        # Training
        model.train()
        train_losses = []
        
        for images, targets in train_loader:
            images = images.to(DEVICE)
            labels = targets['labels'][:, 0] if targets['labels'].size(1) > 0 else torch.zeros(images.size(0), dtype=torch.long)
            labels = labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
        
        train_loss = np.mean(train_losses)
        
        # Validation
        val_loss = evaluate(model, val_loader, criterion)
        
        # Scheduler step
        scheduler.step(val_loss)
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        
        if val_loss < history['best_val_loss']:
            history['best_val_loss'] = val_loss
            history['best_epoch'] = epoch
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
            model_path = MODELS_DIR / f'trial_{trial_number}_best.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'config': config
            }, model_path)
        
        epoch_time = time.time() - epoch_start
        log_print(f"   Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f} ({epoch_time:.1f}s)")
    
    total_time = time.time() - start_time
    log_print(f"\n‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time/60:.2f} —Ö–≤")
    log_print(f"üèÜ –ù–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: val_loss={history['best_val_loss']:.4f} (epoch {history['best_epoch']+1})")
    
    # –§—ñ–Ω–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
    final_metrics = {
        'trial_number': trial_number,
        'final_val_loss': history['val_loss'][-1],
        'best_val_loss': history['best_val_loss'],
        'best_epoch': history['best_epoch'],
        'training_time_minutes': total_time / 60,
        'history': history
    }
    
    return final_metrics


# ============================================================================
# MAIN
# ============================================================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–æ–ø-3"""
    
    # Setup
    setup_logging()
    log_print(f"üéØ –ü–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–æ–ø-{TOP_K} –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä")
    log_print(f"   FULL_EPOCHS: {FULL_EPOCHS}")
    log_print(f"   DEVICE: {DEVICE_NAME}")
    log_print(f"   SEED: {SEED}")
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –¥–ª—è –º–æ–¥–µ–ª–µ–π
    MODELS_DIR.mkdir(exist_ok=True)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Optuna study
    if not CHECKPOINT_FILE.exists():
        log_print(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª study: {CHECKPOINT_FILE}")
        log_print(f"   –°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç—ñ—Ç—å synthesis_universal.py")
        return
    
    log_print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Optuna study –∑ {CHECKPOINT_FILE}")
    with open(CHECKPOINT_FILE, 'rb') as f:
        study = pickle.load(f)
    
    log_print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(study.trials)} trials")
    
    # –í—ñ–¥–±—ñ—Ä —Ç–æ–ø-K
    trials_sorted = sorted(study.trials, key=lambda t: t.value)
    top_trials = trials_sorted[:TOP_K]
    
    log_print(f"\nüìä –¢–æ–ø-{TOP_K} –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è:")
    for i, trial in enumerate(top_trials, 1):
        log_print(f"   #{i} Trial {trial.number}: proxy={trial.value:.4f}")
    
    # –ü–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
    all_results = []
    
    for i, trial in enumerate(top_trials, 1):
        log_print(f"\n{'#'*60}")
        log_print(f"# –ú–û–î–ï–õ–¨ {i}/{TOP_K}")
        log_print(f"{'#'*60}")
        
        try:
            metrics = train_full_model(trial.params, trial.number, epochs=FULL_EPOCHS)
            metrics['rank'] = i
            metrics['proxy_value'] = trial.value
            all_results.append(metrics)
        except Exception as e:
            log_print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—ñ Trial #{trial.number}: {e}")
            continue
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    final_results = {
        'models': all_results,
        'metadata': {
            'top_k': TOP_K,
            'full_epochs': FULL_EPOCHS,
            'device': DEVICE_NAME,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    }
    
    with open(FINAL_RESULTS_FILE, 'w') as f:
        json.dump(final_results, f, indent=2)
    
    # –ü—ñ–¥—Å—É–º–æ–∫
    log_print(f"\n{'='*60}")
    log_print("üèÅ –§–Ü–ù–ê–õ–¨–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    log_print(f"{'='*60}")
    
    # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ best_val_loss
    all_results_sorted = sorted(all_results, key=lambda r: r['best_val_loss'])
    
    for i, result in enumerate(all_results_sorted, 1):
        log_print(f"\nü•á –ú—ñ—Å—Ü–µ #{i}: Trial {result['trial_number']}")
        log_print(f"   Proxy value: {result['proxy_value']:.4f}")
        log_print(f"   Best val loss: {result['best_val_loss']:.4f} (epoch {result['best_epoch']+1})")
        log_print(f"   Training time: {result['training_time_minutes']:.2f} —Ö–≤")
    
    log_print(f"\n‚úÖ –í—Å—ñ –º–æ–¥–µ–ª—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É: {MODELS_DIR}")
    log_print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏: {FINAL_RESULTS_FILE}")
    
    if LOG_FILE:
        log_print(f"üìù –õ–æ–≥: {LOG_FILE}")


if __name__ == '__main__':
    main()
