"""
Stability-Aware Proxy Ð´Ð»Ñ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð½Ð¾Ð³Ð¾ Bayesian Optimization
Detection Stability Score (DSS) Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð½Ñ‚ÐµÐ·Ñƒ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ð¸Ñ… CNN-Ð´ÐµÑ‚ÐµÐºÑ‚Ð¾Ñ€Ñ–Ð²

ÐÐ²Ñ‚Ð¾Ñ€: ÐÐ½Ð°Ñ‚Ð¾Ð»Ñ–Ð¹ ÐšÐ¾Ñ‚
Ð”Ð°Ñ‚Ð°: 2026-01-24
"""

import os
import sys
import json
import time
import random
import pickle
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Optional, Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import transforms
from PIL import Image
import optuna
from optuna.samplers import TPESampler


# ============================================================================
# ÐšÐžÐÐ¤Ð†Ð“Ð£Ð ÐÐ¦Ð†Ð¯
# ============================================================================

SEED = 42
N_TRIALS = 30
N_WARMUP = 10
EPOCHS_PER_TRIAL = 1
K_BATCHES = 10

MAX_SAMPLES = 700
VAL_SUBSET = 200
IMG_SIZE = 320

RESULTS_DIR = Path('results')
CHECKPOINT_FILE = RESULTS_DIR / 'optuna_study.pkl'
STATS_FILE = RESULTS_DIR / 'proxy_stats.json'
RESULTS_FILE = RESULTS_DIR / 'synthesis_results.json'

# ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡Ð½Ðµ Ð²Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð¸
try:
    import google.colab
    IS_COLAB = True
    DATA_ROOT = Path('/content/data')
    DRIVE_ROOT = Path('/content/drive/MyDrive/Studying/Experiments/Composite_score_nas')
except ImportError:
    IS_COLAB = False
    DATA_ROOT = Path('data')
    DRIVE_ROOT = None

# Ð’Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ device
if torch.cuda.is_available():
    DEVICE = torch.device('cuda')
    DEVICE_NAME = 'CUDA'
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    DEVICE = torch.device('mps')
    DEVICE_NAME = 'MPS'
else:
    DEVICE = torch.device('cpu')
    DEVICE_NAME = 'CPU'


# ============================================================================
# LOGGING
# ============================================================================

LOG_FILE = None

def setup_logging():
    """ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ Ð»Ð¾Ð³ÑƒÐ²Ð°Ð½Ð½Ñ Ð· UTC timestamps"""
    global LOG_FILE
    RESULTS_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
    LOG_FILE = RESULTS_DIR / f'experiment_{timestamp}.log'
    log_print(f"ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñƒ: {timestamp} UTC")
    log_print(f"ðŸ“ Platform: {DEVICE_NAME} | Colab: {IS_COLAB}")

def log_print(msg: str):
    """Ð’Ð¸Ð²Ñ–Ð´ Ð· UTC timestamp Ñƒ ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ Ñ‚Ð° Ñ„Ð°Ð¹Ð»"""
    timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
    formatted = f"[{timestamp}] {msg}"
    print(formatted)
    if LOG_FILE:
        with open(LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(formatted + '\n')


# ============================================================================
# DATASET
# ============================================================================

class VisDroneDataset(Dataset):
    """VisDrone2019-DET Dataset Ð´Ð»Ñ Ð´ÐµÑ‚ÐµÐºÑ†Ñ–Ñ— Ð¾Ð±'Ñ”ÐºÑ‚Ñ–Ð²"""
    
    def __init__(self, root: Path, split: str = 'train', transform=None):
        self.root = root / split
        self.images_dir = self.root / 'images'
        self.annotations_dir = self.root / 'annotations'
        self.transform = transform
        
        self.image_files = sorted(self.images_dir.glob('*.jpg'))
        log_print(f"ðŸ“¦ Dataset {split}: {len(self.image_files)} Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½ÑŒ")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        image = Image.open(img_path).convert('RGB')
        
        # Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð°Ð½Ð¾Ñ‚Ð°Ñ†Ñ–Ð¹ (Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚: bbox_left, bbox_top, bbox_width, bbox_height, score, category, ...)
        ann_path = self.annotations_dir / img_path.with_suffix('.txt').name
        boxes = []
        labels = []
        
        if ann_path.exists():
            with open(ann_path, 'r') as f:
                for line in f:
                    parts = line.strip().split(',')
                    if len(parts) >= 6:
                        x, y, w, h, score, category = map(int, parts[:6])
                        if score > 0 and category in range(1, 11):  # 10 ÐºÐ»Ð°ÑÑ–Ð²
                            boxes.append([x, y, x+w, y+h])
                            labels.append(category)
        
        if self.transform:
            image = self.transform(image)
        
        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð°Ñ†Ñ–Ñ Ñƒ Ñ‚ÐµÐ½Ð·Ð¾Ñ€Ð¸
        if len(boxes) == 0:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.long)
        else:
            boxes = torch.tensor(boxes, dtype=torch.float32)
            labels = torch.tensor(labels, dtype=torch.long)
        
        return image, {'boxes': boxes, 'labels': labels}


def get_dataloaders(batch_size: int) -> Tuple[DataLoader, DataLoader]:
    """Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ DataLoader'Ñ–Ð² Ð´Ð»Ñ train/val"""
    
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Train dataset (Ð¿Ñ–Ð´Ð¼Ð½Ð¾Ð¶Ð¸Ð½Ð° Ð´Ð»Ñ ÑˆÐ²Ð¸Ð´ÐºÐ¾ÑÑ‚Ñ–)
    train_dataset = VisDroneDataset(DATA_ROOT, split='train', transform=transform)
    train_indices = list(range(min(MAX_SAMPLES, len(train_dataset))))
    train_subset = Subset(train_dataset, train_indices)
    
    # Val dataset (Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð° Ð¿Ñ–Ð´Ð¼Ð½Ð¾Ð¶Ð¸Ð½Ð°)
    val_dataset = VisDroneDataset(DATA_ROOT, split='val', transform=transform)
    val_indices = list(range(min(VAL_SUBSET, len(val_dataset))))
    val_subset = Subset(val_dataset, val_indices)
    
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, 
                             num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False,
                           num_workers=2, pin_memory=True)
    
    return train_loader, val_loader


# ============================================================================
# MODEL
# ============================================================================

class DynamicDetector(nn.Module):
    """Ð”Ð¸Ð½Ð°Ð¼Ñ–Ñ‡Ð½Ð¸Ð¹ CNN-Ð´ÐµÑ‚ÐµÐºÑ‚Ð¾Ñ€ Ð· Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸Ð·Ð¾Ð²Ð°Ð½Ð¾ÑŽ Ð°Ñ€Ñ…Ñ–Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð¾ÑŽ"""
    
    def __init__(self, config: Dict):
        super().__init__()
        self.config = config
        
        # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸
        n_blocks = config['n_blocks']
        filter_sizes = config['filter_sizes']
        kernel_sizes = config['kernel_sizes']
        fc_size = config['fc_size']
        dropout = config['dropout']
        activation = config['activation']
        
        # Activation function
        if activation == 'relu':
            act_fn = nn.ReLU
        elif activation == 'leaky_relu':
            act_fn = nn.LeakyReLU
        else:  # gelu
            act_fn = nn.GELU
        
        # Convolutional blocks
        layers = []
        in_channels = 3
        
        for i in range(n_blocks):
            out_channels = filter_sizes[i]
            kernel = kernel_sizes[i]
            padding = kernel // 2
            
            layers.extend([
                nn.Conv2d(in_channels, out_channels, kernel, padding=padding),
                nn.BatchNorm2d(out_channels),
                act_fn(),
                nn.MaxPool2d(2)
            ])
            in_channels = out_channels
        
        self.features = nn.Sequential(*layers)
        
        # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ñ€Ð¾Ð·Ð¼Ñ–Ñ€Ñƒ Ð¿Ñ–ÑÐ»Ñ conv-Ð±Ð»Ð¾ÐºÑ–Ð²
        feature_size = IMG_SIZE // (2 ** n_blocks)
        flat_size = in_channels * feature_size * feature_size
        
        # Detection head
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(flat_size, fc_size),
            act_fn(),
            nn.Dropout(dropout),
            nn.Linear(fc_size, 10)  # 10 ÐºÐ»Ð°ÑÑ–Ð² VisDrone
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# ============================================================================
# TRAINING
# ============================================================================

def train_epoch(model: nn.Module, loader: DataLoader, optimizer, criterion,
                track_stability: bool = False) -> Dict[str, float]:
    """Ð¢Ñ€ÐµÐ½ÑƒÐ²Ð°Ð½Ð½Ñ Ð¾Ð´Ð½Ñ–Ñ”Ñ— ÐµÐ¿Ð¾Ñ…Ð¸ Ð· Ð¾Ð¿Ñ†Ñ–Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¸Ð¼ tracking ÑÑ‚Ð°Ð±Ñ–Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–"""
    
    model.train()
    losses = []
    grad_norms = []
    
    for batch_idx, (images, targets) in enumerate(loader):
        images = images.to(DEVICE)
        # Ð¡Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð° loss: CrossEntropy Ð½Ð° Ð¿ÐµÑ€ÑˆÐ¾Ð¼Ñƒ bbox
        labels = targets['labels'][:, 0] if targets['labels'].size(1) > 0 else torch.zeros(images.size(0), dtype=torch.long)
        labels = labels.to(DEVICE)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        
        # Gradient norm
        if track_stability:
            total_norm = 0.0
            for p in model.parameters():
                if p.grad is not None:
                    total_norm += p.grad.data.norm(2).item() ** 2
            grad_norms.append(total_norm ** 0.5)
        
        optimizer.step()
        losses.append(loss.item())
        
        # ÐžÐ±Ð¼ÐµÐ¶ÐµÐ½Ð½Ñ Ð´Ð»Ñ K_BATCHES
        if track_stability and batch_idx >= K_BATCHES - 1:
            break
    
    metrics = {'loss_mean': np.mean(losses)}
    
    if track_stability and len(losses) > 1:
        metrics['loss_std'] = np.std(losses)
        metrics['loss_cv'] = metrics['loss_std'] / (metrics['loss_mean'] + 1e-8)
        metrics['grad_norm_mean'] = np.mean(grad_norms)
        metrics['grad_norm_std'] = np.std(grad_norms)
        metrics['grad_cv'] = metrics['grad_norm_std'] / (metrics['grad_norm_mean'] + 1e-8)
    
    return metrics


def evaluate(model: nn.Module, loader: DataLoader, criterion) -> float:
    """ÐžÑ†Ñ–Ð½ÐºÐ° Ð½Ð° Ð²Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ—"""
    model.eval()
    losses = []
    
    with torch.no_grad():
        for images, targets in loader:
            images = images.to(DEVICE)
            labels = targets['labels'][:, 0] if targets['labels'].size(1) > 0 else torch.zeros(images.size(0), dtype=torch.long)
            labels = labels.to(DEVICE)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            losses.append(loss.item())
    
    return np.mean(losses)


# ============================================================================
# PROXY STATISTICS
# ============================================================================

class ProxyStatistics:
    """Robust-ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð´Ð»Ñ z-Ð½Ð¾Ñ€Ð¼Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ— DSS ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ–Ð²"""
    
    def __init__(self):
        self.stats = {}
        self.warmup_data = {
            'impr': [],
            'L_val': [],
            'loss_cv': [],
            'grad_cv': [],
            'gap': [],
            'L_tr_last': [],
            'grad_norm_mean': []
        }
    
    def add_warmup_trial(self, metrics: Dict[str, float]):
        """Ð”Ð¾Ð´Ð°Ñ‚Ð¸ trial Ñƒ warmup-Ñ„Ð°Ð·Ñƒ"""
        for key in self.warmup_data:
            if key in metrics:
                self.warmup_data[key].append(metrics[key])
    
    def calibrate(self):
        """ÐšÐ°Ð»Ñ–Ð±Ñ€ÑƒÐ²Ð°Ñ‚Ð¸ robust-ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ (median + IQR)"""
        for key, values in self.warmup_data.items():
            if len(values) > 0:
                median = np.median(values)
                q25 = np.percentile(values, 25)
                q75 = np.percentile(values, 75)
                iqr = q75 - q25
                self.stats[key] = {
                    'median': float(median),
                    'iqr': float(iqr if iqr > 1e-8 else 1.0)
                }
        
        log_print(f"ðŸ“Š ÐšÐ°Ð»Ñ–Ð±Ñ€Ð¾Ð²Ð°Ð½Ð¾ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð½Ð° {len(self.warmup_data['L_val'])} warmup trials")
    
    def z_normalize(self, key: str, value: float) -> float:
        """Robust z-Ð½Ð¾Ñ€Ð¼Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ"""
        if key not in self.stats:
            return 0.0
        median = self.stats[key]['median']
        iqr = self.stats[key]['iqr']
        return (value - median) / iqr
    
    def save(self, path: Path):
        """Ð—Ð±ÐµÑ€ÐµÐ³Ñ‚Ð¸ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ"""
        with open(path, 'w') as f:
            json.dump(self.stats, f, indent=2)
    
    def load(self, path: Path):
        """Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ñ‚Ð¸ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ"""
        with open(path, 'r') as f:
            self.stats = json.load(f)


# ============================================================================
# DETECTION STABILITY SCORE (DSS)
# ============================================================================

def compute_dss(metrics: Dict[str, float], stats: ProxyStatistics) -> Tuple[float, str]:
    """
    ÐžÐ±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Detection Stability Score (DSS)
    
    Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð°:
    DSS = 0.25Â·z(impr) + 0.20Â·z(L_val) + 0.15Â·z(loss_cv) + 0.15Â·z(grad_cv) + 
          0.15Â·z(gap) + 0.05Â·z(L_tr) + 0.05Â·z(grad_norm)
    
    ÐšÐ»ÑŽÑ‡Ð¾Ð²Ñ– Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð¸:
    - Ð’Ð¡Ð† Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¿Ð¾Ð·Ð¸Ñ‚Ð¸Ð²Ð½Ð¾ ÐºÐ¾Ñ€ÐµÐ»ÑŽÑŽÑ‚ÑŒ Ð· final loss (Ñ‡Ð¸Ð¼ Ð¼ÐµÐ½ÑˆÐµ - Ñ‚Ð¸Ð¼ ÐºÑ€Ð°Ñ‰Ðµ)
    - z-Ð½Ð¾Ñ€Ð¼Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ð´Ð»Ñ ÐºÐ¾Ñ€ÐµÐºÑ‚Ð½Ð¾Ð³Ð¾ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ñƒ
    - Ð’Ð°Ð³Ð¸ Ð±Ð°Ð·ÑƒÑŽÑ‚ÑŒÑÑ Ð½Ð° ÐºÐ¾Ñ€ÐµÐ»ÑÑ†Ñ–Ð¹Ð½Ð¾Ð¼Ñƒ Ð°Ð½Ð°Ð»Ñ–Ð·Ñ–
    """
    
    # Z-Ð½Ð¾Ñ€Ð¼Ð°Ð»Ñ–Ð·Ð¾Ð²Ð°Ð½Ñ– ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¸
    z_impr = stats.z_normalize('impr', metrics['impr'])
    z_L_val = stats.z_normalize('L_val', metrics['L_val'])
    z_loss_cv = stats.z_normalize('loss_cv', metrics['loss_cv'])
    z_grad_cv = stats.z_normalize('grad_cv', metrics['grad_cv'])
    z_gap = stats.z_normalize('gap', metrics['gap'])
    z_L_tr_last = stats.z_normalize('L_tr_last', metrics['L_tr_last'])
    z_grad_norm = stats.z_normalize('grad_norm_mean', metrics['grad_norm_mean'])
    
    # ÐšÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ‚Ð½Ð¸Ð¹ score
    dss = (
        0.25 * z_impr +
        0.20 * z_L_val +
        0.15 * z_loss_cv +
        0.15 * z_grad_cv +
        0.15 * z_gap +
        0.05 * z_L_tr_last +
        0.05 * z_grad_norm
    )
    
    return dss, 'DSS'


# ============================================================================
# OBJECTIVE FUNCTION
# ============================================================================

PROXY_STATS = ProxyStatistics()
TRIAL_COUNTER = 0

def objective(trial: optuna.Trial) -> float:
    """Optuna objective function Ð· DSS"""
    
    global TRIAL_COUNTER
    TRIAL_COUNTER += 1
    is_warmup = TRIAL_COUNTER <= N_WARMUP
    
    log_print(f"\n{'='*60}")
    log_print(f"ðŸ” Trial {TRIAL_COUNTER}/{N_TRIALS} {'(WARMUP)' if is_warmup else ''}")
    
    # Ð¡ÐµÐ¼Ð¿Ð»ÑŽÐ²Ð°Ð½Ð½Ñ Ð³Ñ–Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ–Ð²
    n_blocks = trial.suggest_int('n_blocks', 2, 5)
    filter_sizes = [trial.suggest_categorical(f'filter_{i}', [16, 32, 64, 128]) 
                    for i in range(n_blocks)]
    kernel_sizes = [trial.suggest_categorical(f'kernel_{i}', [3, 5]) 
                   for i in range(n_blocks)]
    fc_size = trial.suggest_categorical('fc_size', [32, 64, 128])
    dropout = trial.suggest_categorical('dropout', [0.3, 0.5, 0.7])
    activation = trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'gelu'])
    
    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'adamw', 'sgd'])
    lr = trial.suggest_categorical('lr', [0.0001, 0.001, 0.01])
    weight_decay = trial.suggest_categorical('weight_decay', [0, 1e-5, 1e-4, 1e-3])
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    
    config = {
        'n_blocks': n_blocks,
        'filter_sizes': filter_sizes,
        'kernel_sizes': kernel_sizes,
        'fc_size': fc_size,
        'dropout': dropout,
        'activation': activation
    }
    
    log_print(f"ðŸ—ï¸  Architecture: {n_blocks} blocks, filters={filter_sizes}, kernels={kernel_sizes}")
    log_print(f"âš™ï¸  Optimizer: {optimizer_name.upper()} (LR={lr}, WD={weight_decay}), BS={batch_size}")
    
    # Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ–
    model = DynamicDetector(config).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    
    if optimizer_name == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)
    
    # Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ…
    train_loader, val_loader = get_dataloaders(batch_size)
    
    # ÐŸÐ¾Ñ‡Ð°Ñ‚ÐºÐ¾Ð²Ð° Ð²Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ
    initial_val_loss = evaluate(model, val_loader, criterion)
    
    # Ð¢Ñ€ÐµÐ½ÑƒÐ²Ð°Ð½Ð½Ñ Ð· tracking ÑÑ‚Ð°Ð±Ñ–Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–
    train_metrics = train_epoch(model, train_loader, optimizer, criterion, track_stability=True)
    
    # Ð¤Ñ–Ð½Ð°Ð»ÑŒÐ½Ð° Ð²Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ
    final_val_loss = evaluate(model, val_loader, criterion)
    
    # Ð Ð¾Ð·Ñ€Ð°Ñ…ÑƒÐ½Ð¾Ðº Ð¼ÐµÑ‚Ñ€Ð¸Ðº DSS
    metrics = {
        'impr': initial_val_loss - final_val_loss,  # ÐŸÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð½Ñ (Ñ‡Ð¸Ð¼ Ð±Ñ–Ð»ÑŒÑˆÐµ - Ñ‚Ð¸Ð¼ ÐºÑ€Ð°Ñ‰Ðµ Ð½Ð°Ð²Ñ‡Ð°Ð½Ð½Ñ)
        'L_val': final_val_loss,
        'L_tr_last': train_metrics['loss_mean'],
        'gap': final_val_loss - train_metrics['loss_mean'],
        'loss_cv': train_metrics.get('loss_cv', 0.0),
        'grad_cv': train_metrics.get('grad_cv', 0.0),
        'grad_norm_mean': train_metrics.get('grad_norm_mean', 0.0)
    }
    
    log_print(f"ðŸ“ˆ Metrics: L_val={metrics['L_val']:.4f}, impr={metrics['impr']:.4f}, gap={metrics['gap']:.4f}")
    
    # Warmup Ð°Ð±Ð¾ DSS
    if is_warmup:
        PROXY_STATS.add_warmup_trial(metrics)
        proxy_value = final_val_loss
        proxy_name = 'val_loss'
        log_print(f"ðŸ”¥ Warmup proxy: {proxy_value:.4f}")
        
        # ÐšÐ°Ð»Ñ–Ð±Ñ€ÑƒÐ²Ð°Ñ‚Ð¸ Ð¿Ñ–ÑÐ»Ñ Ð¾ÑÑ‚Ð°Ð½Ð½ÑŒÐ¾Ð³Ð¾ warmup
        if TRIAL_COUNTER == N_WARMUP:
            PROXY_STATS.calibrate()
            PROXY_STATS.save(STATS_FILE)
    else:
        proxy_value, proxy_name = compute_dss(metrics, PROXY_STATS)
        log_print(f"âœ¨ DSS: {proxy_value:.4f}")
    
    # Optuna Ð¼Ñ–Ð½Ñ–Ð¼Ñ–Ð·ÑƒÑ”, Ñ‚Ð¾Ð¼Ñƒ Ð´Ð»Ñ DSS (Ð±Ñ–Ð»ÑŒÑˆÐµ = ÐºÑ€Ð°Ñ‰Ðµ) Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ”Ð¼Ð¾ -DSS
    return -proxy_value if proxy_name == 'DSS' else proxy_value


# ============================================================================
# MAIN
# ============================================================================

def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð¸Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ ÑÐ¸Ð½Ñ‚ÐµÐ·Ñƒ"""
    
    # Setup
    setup_logging()
    set_seed(SEED)
    RESULTS_DIR.mkdir(exist_ok=True)
    
    log_print(f"ðŸŽ¯ ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸ ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñƒ:")
    log_print(f"   - N_TRIALS: {N_TRIALS}")
    log_print(f"   - N_WARMUP: {N_WARMUP}")
    log_print(f"   - EPOCHS_PER_TRIAL: {EPOCHS_PER_TRIAL}")
    log_print(f"   - MAX_SAMPLES: {MAX_SAMPLES}")
    log_print(f"   - VAL_SUBSET: {VAL_SUBSET}")
    log_print(f"   - SEED: {SEED}")
    log_print(f"   - DEVICE: {DEVICE_NAME}")
    
    # Optuna study
    sampler = TPESampler(seed=SEED, n_startup_trials=N_WARMUP)
    study = optuna.create_study(direction='minimize', sampler=sampler)
    
    start_time = time.time()
    
    try:
        study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)
    except KeyboardInterrupt:
        log_print("\nâš ï¸  Ð•ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚ Ð¿ÐµÑ€ÐµÑ€Ð²Ð°Ð½Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ÐµÐ¼")
    
    elapsed = time.time() - start_time
    log_print(f"\nâ±ï¸  Ð§Ð°Ñ ÑÐ¸Ð½Ñ‚ÐµÐ·Ñƒ: {elapsed/60:.2f} Ñ…Ð²Ð¸Ð»Ð¸Ð½")
    
    # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð²
    with open(CHECKPOINT_FILE, 'wb') as f:
        pickle.dump(study, f)
    
    # ÐÐ½Ð°Ð»Ñ–Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð²
    analyze_results(study)
    
    log_print(f"\nâœ… Ð•ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
    log_print(f"ðŸ“Š Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¾ Ñƒ: {RESULTS_DIR}")
    
    if LOG_FILE:
        log_print(f"ðŸ“ Ð›Ð¾Ð³: {LOG_FILE}")


def analyze_results(study: optuna.Study):
    """ÐÐ½Ð°Ð»Ñ–Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð² ÑÐ¸Ð½Ñ‚ÐµÐ·Ñƒ"""
    
    log_print(f"\n{'='*60}")
    log_print("ðŸ“Š ÐÐÐÐ›Ð†Ð— Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð†Ð’")
    log_print(f"{'='*60}")
    
    # ÐÐ°Ð¹ÐºÑ€Ð°Ñ‰Ð¸Ð¹ trial
    best_trial = study.best_trial
    log_print(f"\nðŸ† ÐÐ°Ð¹ÐºÑ€Ð°Ñ‰Ð¸Ð¹ Trial #{best_trial.number}")
    log_print(f"   Proxy value: {best_trial.value:.4f}")
    log_print(f"   ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸:")
    for key, value in best_trial.params.items():
        log_print(f"      {key}: {value}")
    
    # Ð¢Ð¾Ð¿-3
    log_print(f"\nðŸ¥‡ Ð¢Ð¾Ð¿-3 Ð°Ñ€Ñ…Ñ–Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð¸:")
    trials_sorted = sorted(study.trials, key=lambda t: t.value)
    for i, trial in enumerate(trials_sorted[:3], 1):
        log_print(f"\n   #{i} Trial {trial.number}: {trial.value:.4f}")
        n_blocks = trial.params['n_blocks']
        filters = [trial.params[f'filter_{j}'] for j in range(n_blocks)]
        kernels = [trial.params[f'kernel_{j}'] for j in range(n_blocks)]
        log_print(f"      Architecture: {n_blocks} blocks, filters={filters}, kernels={kernels}")
        log_print(f"      Optimizer: {trial.params['optimizer'].upper()} (LR={trial.params['lr']}, WD={trial.params['weight_decay']})")
        log_print(f"      Activation: {trial.params['activation']}, Dropout: {trial.params['dropout']}")
    
    # Ð—Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð½Ñ Ñƒ JSON
    results = {
        'best_trial': {
            'number': best_trial.number,
            'value': best_trial.value,
            'params': best_trial.params
        },
        'top3': [
            {
                'number': t.number,
                'value': t.value,
                'params': t.params
            }
            for t in trials_sorted[:3]
        ],
        'metadata': {
            'n_trials': len(study.trials),
            'device': DEVICE_NAME,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    }
    
    with open(RESULTS_FILE, 'w') as f:
        json.dump(results, f, indent=2)


def set_seed(seed: int):
    """Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ seed Ð´Ð»Ñ Ð²Ñ–Ð´Ñ‚Ð²Ð¾Ñ€ÑŽÐ²Ð°Ð½Ð¾ÑÑ‚Ñ–"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


if __name__ == '__main__':
    main()
